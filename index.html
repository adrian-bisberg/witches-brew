<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Witches Brew</title>
  <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Merriweather:wght@400;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="style.css">

  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</head>

<body>
  <div class="blog-container">
    <h1 class="blog-title">An Analysis of: Witches&rsquo; Brew: Industrial Scale Data Poisoning Via Gradient Matching
    </h1>

    <h4>Analysis by Adrian Bisberg and Evan Rose</h4>

    <section class="blog-section magic-box" id="introduction">
      <h2>Introduction</h2>

      <p>
        Modern machine learning typically relies on collecting massive amounts of training data, often from untrusted sources, in order to train a large-scale model. For example, the Pile dataset (<a href="#cite-gao2020">Gao et al., 2020</a>), which is commonly used for language modeling tasks, consists of over 800 GB of text data scraped from the internet with only limited filtering. This process exposes machine learning systems to <i>data poisoning attacks</i>. In a data poisoning attack, an adversary injects malicious data into the training set in order to manipulate the behavior of the trained model.
      </p>

      <!-- Witches&rsquo; Brew introduces a novel type of data poisoning attack where a model is trained from scratch using tampered data in order to target a specific image for misclassification. The authors are motivated by creating an integrity attack that improves upon previous work to target larger scale models and work with minimal amounts of poisoned training data. -->

      <p>
        This paper studies a stealthy type of poisoning attack in which the poisoned images must resemble clean training data. This means the attacker cannot simply inject incorrectly-labeled examples into the dataset, but rather must apply imperceptible perturbations on top of the original images. To the human eye, the poisoned dataset is indistinguishable from the original.
      </p>

      <figure class="fig1">
        <img src="media/poisoning_overview.png" alt="An overview of the poisoning procedure" class="centered-image">
        <figcaption class="figcaption1">
          The attack perturbs a small set of data (between 0.1% and 1% of the training set) in an imperceptable manner using parameter information from a pre-trained model. After re-training from scratch on the poisoned data, the model misclassifies the target image as the adversarial label.
        </figcaption>
      </figure>
    </section>

    <section class="blog-section magic-box" id="methods">
      <h2>Methods and Results</h2>

      <h3>Threat Model</h3>

      <p>
        Security papers typically outline a set of rules called a <i>threat model</i> describing the knowledge, capabilities, and goals of the attacker. The threat model used in Witches&rsquo; Brew is as follows:
      </p>

      <ul>
        <li>
          <strong>Goal:</strong> The attacker wishes to force the model to misclassify a specific image (or a small set of images) \(x^t\) to a particular class \(y^{\mathrm{adv}}\).
        </li>
        <li>
          <strong>Knowledge:</strong> The attacker may have knowledge of the model architecture, and potentially some small fraction of the training data.
        <li>
          <strong>Capabilities:</strong> The attacker can modify a small number of training samples, but cannot modify their labels. Moreover, the attacker can only modify each samples by a small amount, so that poisoned data is indistinguishable from clean data to the human eye.
        </li>
      </ul>

      <p>
        Formally, this threat model can be described as a bi-level optimization problem in which the adversary wants to minimize an <i>adversarial loss</i> (the adversary's objective) subject to the model developer's training procedure (on poisoned data):
      </p>

      <p>
        \begin{align}
        \min_{\Delta \in \mathcal{C}} &\sum_{i=1}^T \mathcal{L}(F(x_i^t, \theta(\Delta)), y_i^{\mathrm{adv}}) \\
        &\mathrm{s.t.} \; \theta(\Delta) \in \arg \min_{\theta} \frac{1}{N} \sum_{i=1}^N \mathcal{L}(F(x_i + \Delta_i, \theta), y_i)
        \end{align}
      </p>

      <p>
        In this formulation, the (bounded) perturbation \(\Delta\) is optimized
        to enforce the malicious label onto target data, while the solution to the inner problem still minimizes the original training objective of the model. A per-instance perturbation budget \(\varepsilon\) bounds the \(\ell_\infty\)-norm of the perturbation \(\Delta_i\), and this constraint is encoded inside the set \(\mathcal{C}\).
      </p>

      <h3>Key Idea: Gradient Alignment</h3>

      <p>
        The bi-level problem above is difficult to solve directly due to the expensive nature of the inner problem. Instead, the authors propose a method called <i>gradient alignment</i> to solve the problem efficiently. The key idea is to align the gradient of the adversarial loss with the gradient of the poisoned training loss. If the adversary could enforce similarity of the gradients:
      </p>

      <p>
        \[
        \nabla_\theta \mathcal{L}(F(x^t, \theta), y^{\mathrm{adv}}) \approx \frac{1}{N} \sum_{i=1}^N \nabla_{\theta} \mathcal{L}(F(x_i + \Delta_i, \theta), y_i)
        \]
      </p>

      <p>
        for all \(\theta\) encountered during training, the adversarial loss would correspondingly decrease. Since optimizing over all parameters is infeasible, this is achieved in practice by choosing perturbations \(\Delta_i\) minimizing the cosine dissimilarity between the two gradients over a small number of model parameters:
      </p>

      <p>
        \[
        \min_{\Delta \in \mathcal{C}} \sum_{i=1}^T \left(1 - \frac{\nabla_\theta \mathcal{L}(F(x^t, \theta), y^{\mathrm{adv}}) \cdot \nabla_{\theta} \sum_{i=1}^P \mathcal{L}(F(x_i + \Delta_i, \theta), y_i)}{\|\nabla_\theta \mathcal{L}(F(x^t, \theta), y^{\mathrm{adv}})\|_2 \|\nabla_{\theta} \sum_{i=1}^P \mathcal{L}(F(x_i + \Delta_i, \theta), y_i)\|_2}\right)
        \]
      </p>

      <!-- <figure class="fig1">
        \[\nabla_\theta \mathcal{L}(F(x^t, \theta), y^{\mathrm{adv}}) \approx \frac{1}{P} \sum_{i=1}^P \nabla_{\theta} \mathcal{L}(F(x_i + \Delta_i, \theta), y_i) \]
        <figcaption class="figcaption1"> On the left side: the loss between adversarial label to predicted label of the targeted image. The goal is to minimize This
          loss in order to misclassify the target image as the adversarial label. On the right side: the loss described between
          predicted label of poisoned data and its original label. Minimizing this loss maintains the validation accuracy of the model to
          keep the attack imperceptible.
        </figcaption>
      </figure> -->

      <!-- <figure class="fig1">
        <img src="media/matching_gradient_eq.png" class="centered-image">
      </figure> -->

      <!-- <p>
        Note: Here, authors decide to use a single target image x<sub>t</sub> for their experiments but mentioned multiple could be included
        at once. However evidence of the effectiveness of using the method for multiple targets is not strong.
      </p> -->


      <!-- <p>
        The technique used in their attack is called gradient matching. Here, the attacker mimics the gradient of the target by creating poisoned data whose training gradient aligns with the gradient of an adversarial target.
      </p> -->

      <!-- </p> -->

      <p>
        In order to align the target and the poison gradients in the same direction, the attack minimizes the cosine dissimilarity between them using signed Adam updates with a decaying step size. In this formulation, the model parameters \(\theta\) are not changed, instead \(\Delta_i\) is calculated in order to find the optimal poison to force misclassification. This method is particularly efficient as it only requires time scaling with the number of poisons \(P\), rather than the entire dataset.
      </p>

      <p>
        The authors additionally justify their method on theoretical grounds by invoking a classical result in numerical optimization called <i>Zoutendijk's Theorem</i> (<a href="#cite-nocedal2006">Nocedal & Wright, 2006</a>) to show that their bi-level gradient descent will reach a local optimum of the adversarial loss if the alignment is maintained consistently.
      </p>

      <h3>Experimental Results</h3>

      <figure class="fig1" style="float: right; width: 50%; margin-left: 1em;">
        <img src="media/cosine_similarity.png" class="centered-image">
        <figcaption class="figcaption1"> Plotting the cosine similarity across training epochs, comparing the poisoned image set versus the unmodified
          training to illustrate the gradient alignment achieved by the optimization process. The red line which represents that poisoned data maintains
          a positive similarity throughout training.
        </figcaption>
      </figure>

      <p>
        Because the threat model of the attack does not give full knowledge of the model architecture, such as dropout layers, hyperparameters, data
        augmentation, etc, the authors propose several techniques to make to attack more robust to varying architecture:
      </p>

      <ol>
        <li>
          Data augmentation - at each minimization step, randomly draw a translation crop, and possibly flip for each poisoned image. Creates
          increased transferability and robustness to possible data augmentations within the model.
        </li>
        <li>
          Restarts - minimize the cosine similarity several times using different random starting perturbations, then select the captured alignment loss.
        </li>
        <li>
          Model Ensembles - optimize the attack over an ensemble of different models, increasing generalized performance at a high computational cost.
        </li>
      </ol>


      <figure class="fig1" style="float: right; width: 50%; margin-left: 1em;">
        <img src="media/graph_results.png" class="centered-image">
        <figcaption class="figcaption1"> Results on CIFAR-10 for a ResNet-18 to visualize varied values for R (restarts) and K (number of ensembles).
        </figcaption>
      </figure>

      <p>
        The authors display results for the success rate of the proposed method in producing the targeted misclassification. Overall, the attack is very effective (success rate \(> 90%\)) when the model architecture is known in advance and when the perturbation budget is sufficiently generous (\(\varepsilon \ge 16\)). When restricting the perturbation budget to \(\varepsilon = 8\), the attack success rate drops (45%-55% on CIFAR-10). When the poisons are generated using a surrogate model having a different architecture than the victim, success decreases further, to as low as \(7%\) when transferring from ResNet-18 to VGG11. However, attack generality can be improved by optimizing poisons over an ensemble of heterogeneous model architectures.
      </p>

      <figure class="fig1">
        <img src="media/poisoned_diff.png" class="centered-image">
        <figcaption class="figcaption1">
          Poisoning ImageNet. On the top, clean images from the original dataset. On the bottom, poisoned images after applying optimized perturbations. Poisons were generated with a perturbation budget of \(\varepsilon = 8\).
        </figcaption>
      </figure>

      <figure class="fig1">
        <img src="media/results_compared.png" class="centered-image">
        <figcaption class="figcaption1"> Results using benchmark proposed in Schwarzschild et al. (2020). The category "Training From Scratch" evaluates poisoned CIFAR-10 datasets with a poised training data budget of 1% and \(\epsilon = 8\) averaged over 100 attack targets. Compares the results of previously proposed methods for data poisoning from the same category. Models trained using ResNet-18 are evaluated next to other model architectures to demonstrate the transferability of the attack.
        </figcaption>
      </figure>

      </p>
    </section>

    <section class="blog-section magic-box" id="history">
      <h2>History and Related Works</h2>
      <p>
        Data poisoning attacks against machine learning extend back at least as early as the 2000s, when security researchers were interested in manipulating email spam filters (<a href="#cite-nelson2008">Nelson et al., 2008</a>). This was a time before deep learning had gained traction in practical applications, so these attacks typically targeted simple classifiers, such as logistic regression or SVM.
      </p>

      <p>
        Between 2012 and 2014, a number of works developed gradient-based attacks against classification, including against neural networks. <a href="#cite-biggio2012">Biggio et al.</a> (2012) used gradient methods to craft poisoned data that increases SVM test error. Around the same time, <a href="#cite-biggio2013">Biggio et al.</a> (2013) and <a href="#cite-szegedy2014">Szegedy et al.</a> (2014) used gradient methods to evade machine learning classifiers by perturbing test instances in imperceptible ways. This early work sparked a flurry of work on understanding the robustness properties of machine learning systems, especially deep neural networks.
      </p>

      <p>
        Modern work on poisoning has advanced to become much more effective and stealthy. For example, the early backdoor poisoning attack by <a href="#cite-gu2017">Gu et al.</a> (2017), which assumed an attacker with the ability to arbitrarily change training data, was soon refined to include versions in which the attacker does not need the ability to modify data labels (<a href="#cite-turner2019">Turner et al., 2019</a>) or reveal the backdoor trigger inside the training data (<a href="#cite-saha2020">Saha et al., 2020</a>). Other attacks targeted the transfer learning setting (<a href="#cite-shafahi2018">Shafahi et al, 2018</a>; <a href="#cite-zhu2019">Zhu et al., 2019</a>) by crafting poisons that behavior anomolously in the learned represenatation space.
      </p>

      <p>
        Today, practitioners consider poisoning attacks to be among the most serious threats to machine learning security (<a href="#cite-sivakumar2020">Kumar et al., 2020</a>). This is because modern machine learning typically relies on collecting massive amounts of training data, often from untrusted sources and on such a large scale that manual data cleaning is impossible.
      </p>
    </section>

    <section class="blog-section magic-box" id="author-biography">
      <h2>Author Biography</h2>
      <p>
      <div class="row">
        <img src="media/jonas_geiping.png" class="photo">
        <div class="description">
          <h3>Jonas Geiping</h3>
          <p> Education: MS in Mathematics from University of Münster; PhD in Computer Science from University of Siegen <br>
            Jonas was a Postdoctoral Associate at University of Maryland during the publishing of this paper. <br>
            Research goals: Understanding data privacy and adversarial attacks in large scale models. How to define and develop
            "safer" models.
          </p>
        </div>
      </div>

      <div class="row">
        <img src="media/liam_fowl.jpg" class="photo">
        <div class="description">
          <h3>Liam Fowl</h3>
          <p>Education: PhD in Mathematics from University of Maryland <br>
            Research Interests: Clean-label data poisoning, metalearning, and federated learning</p>
        </div>
      </div>

      <div class="row">
        <img src="media/ronny_huang.jpg" class="photo">
        <div class="description">
          <h3>W Ronny Huang</h3>
          <p>Education: PhD in Electrical Engineering and Computer Science at MIT, Machine Learning Researcher at University of Maryland <br>
            Professional: Technical Staff at NASA, Research Scientist at Google </p>
        </div>
      </div>

      <div class="row">
        <img src="media/empty.jpg" class="photo">
        <div class="description">
          <h3>Wojciech Czaja</h3>
          <p> Professor in the Mathematics department at University of Maryland <br>
            Author on over 100 papers at UMD, several collaborations with Geiping and Fowl</p>
        </div>
      </div>

      <div class="row">
        <img src="media/gavin_taylor.jpg" class="photo">
        <div class="description">
          <h3>Gavin Taylor</h3>
          <p>Education: PhD from Duke University <br>
            Professional: Professor of Computer Science at US Naval Academy, researcher in collaboration with
            UMD on the behaviors and vulnerabilities of neural networks
          </p>
        </div>
      </div>

      <div class="row">
        <img src="media/michael_moller.jpg" class="photo">
        <div class="description">
          <h3>Michael Moeller</h3>
          <p>Education: PhD from University of Münster <br>
            Professional: Professor for Computer Vision at the University of Siegen, Germany <br>
            Research Interests: Model and learning based techniques in imaging and computer vision
          </p>
        </div>
      </div>

      <div class="row">
        <img src="media/tom_goldstein.png" class="photo">
        <div class="description">
          <h3>Tom Goldstein</h3>
          <p>Education: PhD in Mathematics from UCLA, research scientist at Rice University and Stanford University<br>
            Professional: Professor of Computer Science at University of Maryland<br>
            Research Interests: Computer vision, signal processing, platform optimization
          </p>
        </div>
      </div>
      </p>
    </section>

    <section class="blog-section magic-box" id="societal-impact">
      <h2>Societal Impact</h2>

      <p>
        Data poisoning attacks pose a serious security threat to the integrity of learning systems. The authors of Witches&rsquo; Brew demonstrate
        that poising attacks can be successfully launched using only imperceptible changes to images, minimal amounts of modified training data, and no significant effect to the validation accuracy of the system. In machine learning systems that rely on large amounts of web scraped data, attacks like Witches&rsquo; Brew can easily be injected into several training cycles over time, making the attack both hard to detect and difficult to fix.
      </p>

      <p>
        There are always qualifications to the feasibility of an adversarial attack; for Witches&rsquo; Brew the attack does depend on availability of the trained model and access to contributing training data. This means the attack would be more effective against a continual learning model or a reinforcement based model.
      </p>

      <p>
        Model integrity attacks have larger social implications by providing a means to bypass important safety measures that the model is trying to ensure. For example, an important measure taken in a large model could be to identify explicit or violent content, and a data poisoning attack could force malicious classification of certain outputs as safe.
      </p>

      <p>
        Other types of adversarial attacks pose additional threats to model security. In a model extraction attack, the attacker uses queries of the model and observed outputs in order to assemble a duplicate of a previously private model. Model inversion attacks can reveal information from the dataset that was used to train the model, and potentially compromise private information.
      </p>

      <p>
        Research into adversarial attacks is useful for identifying model vulnerabilities to different types of attacks and evaluating the effectiveness of various defensive strategies. When models are built with increased knowledge of different attacks, security measures can be incorporated into training and deployment of the model. In recent years, the field of adversarial research in ML has grown significantly, providing for new standards to evaluate a model's robustness against adversarial attacks. Finding a completely generalized defensive solution is difficult, however creating more fully documented and reproducible attacks doubtlessly will help
        unify and progress defensive strategies.
      </p>
    </section>

    <section class="blog-section magic-box" id="industry-applications">
      <h2>Industry Applications</h2>
      <p>
        The central mechanism used to generate the poisons, called gradient alignment, is a general-purpose relaxation of a difficult bi-level optimization problem. Such problems frequently appear in many different applications, even those not involving any security objective. We will briefly motivate one such example, <i>dataset condensation</i>, and describe how gradient alignment can be used to achieve it.
      </p>

      <p>
        Consider the process of learning a new field of study. When the field is nascent, the task of understanding it falls to researchers, who explore the space inefficiently as they attempt to find what does and does not work. As the field matures, however, the knowledge of the field becomes more concentrated, and the task of learning it becomes easier. This is because the knowledge of the field is condensed into a smaller number of papers, which can be read and understood more quickly than the original research. By the time the information reaches the level of a textbook, the knowledge has been refined so that a new reader can understand decades of work in only a couple of months.
      </p>

      <p>
        Dataset condensation (sometimes called dataset distillation) is the process of achieving this same effect in machine learning. The goal is to take a large dataset and condense it into a smaller one, while preserving the performance of a model trained on the original dataset. This is useful when developers wish to retrain a model (for example, on a new architecture) but do not want to incur the large cost of retraining on a large number of samples.
      </p>

      <p>
        The process of dataset condensation can be formulated as a bi-level optimization problem, similar to the formulation used in Witches&rsquo; Brew. The outer problem, which relates to the condensation task, is to minimize the test error of a model trained on a condensed dataset. The inner problem, which relates to the training task, is to minimize the training error of a model trained on the condensed dataset (which is being optimized itself). The outer problem is difficult to solve directly, because it requires full knowledge of the inner problem. Gradient alignment provides an efficient relaxation to this problem: instead of minimizing the test error directly, we minimize the cosine similarity between the original training gradient and the condensed training gradient over some sampling of model parameters.
      </p>
    </section>

    <section class="blog-section magic-box" id="academic-research">
      <h2>Academic Research Follow-Ups</h2>
      <p>
        The robustness of general machine learning algorithms to data poisoning attacks is still not well-understood. A valuable future research direction would be to characterize precisely in which scenarios and to what extent datasets and learning algorithms are susceptible to poisoning. Some recent work has begun to address this question (<a href="#cite-wang2022">Wang et al., 2022</a>; <a href="#cite-lu2023">Lu et al., 2023</a>), but much more work remains to be done.
      </p>

      <p>
        Another important direction is to develop defenses against data poisoning attacks.
      </p>
    </section>

    <section class="blog-section magic-box" id="academic-research">
      <h2>Peer Review Ratings</h2>

      <h3>Adrian:</h3>

      <p>
        Strengths:
      </p>
      <ul>
        <li>
          Clear conceptual and analytical explanation of gradient matching method used. Provides potential for extended work using the bi-level optimization mechanism becuase of the detailed discussion.
        </li>
        <li>
          Impressive results in the scaling and transferability of the method; using significantly sized datasets like ImageNet, and showing transfered success on black box models like Google Cloud AutoML.
        </li>
        <li>
          Clear that the authors made it a goal to make as least invasive an attack as possible. Clearly motivates future work to advance defensive mechanisms against likewise attacks.
        </li>
      </ul>

      <p>
        Weaknesses:
      </p>

      <ul>
        <li>
          Authors claim that matching multiple target images at once with the same attack method will yeild comperable results, but fail to give convincing proof that this will hold.
        </li>

        <li>
          The 0.1% metric for percentage of poisoned data generally seems impressive, but when considered for a very large dataset like ImageNet it actually seems to be a high volume of data still that will need to be poisoned.
        </li>

        <li>
          Defensive strategies discussed seem to cover basics in the field, but could have further benefitted from a theoretical discussion of what new methods it might take to combat this specific attack.
        </li>
      </ul>

      <p>Rating: 7: Good paper, accept.</p>

      <p>Confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct.</p>

      <h3>Evan:</h3>

      <p>
        Strengths:
      </p>

      <ul>
        <li>
          The ability to craft subtle but effective adversarial perturbations to training data reveals interesting properties of the optimization landscape of machine learning models. Besides the immediate security concern, these findings may be important for understanding the properties of the optimization problems arising in neural networks more generally.
        </li>
        <li>
          The relaxation of the bi-level optimization problem used in this paper is a general-purpose technique that may be useful in other settings, such as dataset condensation.
        </li>
        <li>
          The authors provide a clear and detailed explanation of their method, including a theoretical justification for its effectiveness.
        </li>
      </ul>

      <p>
        Weaknesses:
      </p>

      <ul>
        <li>
          Certain details of the threat model seem unrealistic. For example, it is unclear how the attacker is able to obtain useful enough model parameters used for poison crafting. Also
        </li>
        <li>
          The attack is quite limited in its ability to target multiple targets simultaneously. Even if the attack fails when arbitrary targets are chosen, it would be interesting to see if the attack can be made more effective by choosing targets that are similar to each other.
        </li>
        <li>
          It would have been interesting to see how the attack performs on a wider range of attack objectives, such as backdoor attacks which can co-exist with the original training objective.
        </li>
      </ul>

      <p>Rating: 6: Accept.</p>

      <p>Confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct.</p>


    </section>

    <section class="blog-section magic-box" id="references">
      <h2>References</h2>
      <ul id="references-list">
        <li>
          J. Geiping et al. (2021).
          <i>Witches&rsquo; Brew: Industrial Scale Data Poisoning via Gradient Matching.</i>
          doi: <a href="https://doi.org/10.48550/arXiv.2009.02276">10.48550/arXiv.2009.02276</a>
        </li>
        <li>
          Churilla, M., Vanhoudnos, N., & Beveridge, R. (2023, May 15).
          <i>The Challenge of Adversarial Machine Learning.</i>
          Carnegie Mellon Software Engineering.
        </li>
        <li id="cite-nelson2008">
          B. Nelson et al. <i>Exploiting machine learning to subvert your spam filter</i>.
          In <i>Proceedings of the 1st Usenix Workshop on Large-Scale Exploits and Emergent Threats, in LEET'08</i>.
          USA: USENIX Association, Apr. 2008, pp. 1-9.
        </li>
        <li id="cite-sivakumar2020">
          R. S. Siva Kumar et al. <i>Adversarial Machine Learning-Industry Perspectives</i>.
          In <i>2020 IEEE Security and Privacy Workshops (SPW)</i>, May 2020, pp. 69-75.
          doi: <a href="https://doi.org/10.1109/SPW50608.2020.00028">10.1109/SPW50608.2020.00028</a>
        </li>
        <li id="cite-biggio2012">
          B. Biggio, B. Nelson, and P. Laskov. <i>Poisoning attacks against support vector machines</i>.
          In <i>Proceedings of the 29th International Conference on Machine Learning, in ICML'12</i>.
          Madison, WI, USA: Omnipress, Jun. 2012, pp. 1467-1474.
        </li>
        <li id="cite-biggio2013">
          B. Biggio et al. <i>Evasion Attacks against Machine Learning at Test Time</i>.
          In <i>Machine Learning and Knowledge Discovery in Databases, in Lecture Notes in Computer Science</i>.
          Berlin, Heidelberg: Springer, 2013, pp. 387-402.
          doi: <a href="https://doi.org/10.1007/978-3-642-40994-3_25">10.1007/978-3-642-40994-3_25</a>
        </li>
        <li id="cite-szegedy2014">
          C. Szegedy et al. <i>Intriguing properties of neural networks.</i>
          arXiv, Feb. 19, 2014.
          doi: <a href="https://doi.org/10.48550/arXiv.1312.6199">10.48550/arXiv.1312.6199</a>
        </li>
        <li id="cite-gu2017">
          Tianyu Gu, Brendan Dolan-Gavitt, Siddharth Garg (2017).
          <i>BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain.</i>
          <span class="eprint-info">arXiv preprint, arXiv:1708.06733 [cs.CR], 2017.</span>
        </li>
        <li id="cite-saha2020">
          A. Saha, A. Subramanya, and H. Pirsiavash. <i>Hidden-Trigger Backdoor Attacks</i>.
          34th AAAI Conference on Artificial Intelligence (AAAI) 2020, Apr. 2020.
          Accessed: Dec. 04, 2023.
          [Online]. Available: <a href="https://par.nsf.gov/biblio/10188566-hidden-trigger-backdoor-attacks">https://par.nsf.gov/biblio/10188566-hidden-trigger-backdoor-attacks</a>
        </li>
        <li id="cite-turner2019">
          A. Turner, D. Tsipras, and A. Madry. <i>Label-Consistent Backdoor Attacks.</i>
          arXiv, Dec. 06, 2019.
          doi: <a href="https://doi.org/10.48550/arXiv.1912.02771">10.48550/arXiv.1912.02771</a>
        </li>
        <li id="cite-zhu2019">
          C. Zhu, W. R. Huang, H. Li, G. Taylor, C. Studer, and T. Goldstein.
          <i>Transferable Clean-Label Poisoning Attacks on Deep Neural Nets.</i>
          In <i>Proceedings of the 36th International Conference on Machine Learning, PMLR</i>, May 2019, pp. 7614-7623.
          Accessed: Dec. 04, 2023.
          [Online]. Available: <a href="https://proceedings.mlr.press/v97/zhu19a.html">https://proceedings.mlr.press/v97/zhu19a.html</a>
        </li>
        <li id="cite-shafahi2018">
          A. Shafahi et al.
          <i>Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks.</i>
          In <i>Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS'18</i>.
          Red Hook, NY, USA: Curran Associates Inc., Dec. 2018, pp. 6106-6116.
        </li>
        <li id="cite-wang2022">
          W. Wang, A. Levine, and S. Feizi (2022).
          <i>Lethal Dose Conjecture on Data Poisoning.</i>
          In <i>Advances in Neural Information Processing Systems</i>, Dec. 2022, pp. 1776-1789.
          Accessed: Dec. 05, 2023.
          [Online]. Available: <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/0badcb4e95306df76a719409155e46e8-Abstract-Conference.html">https://proceedings.neurips.cc/paper_files/paper/2022/hash/0badcb4e95306df76a719409155e46e8-Abstract-Conference.html</a>
        </li>
        <li id="cite-lu2023">
          Y. Lu, G. Kamath, and Y. Yu (2023).
          <i>Exploring the Limits of Model-Targeted Indiscriminate Data Poisoning Attacks.</i>
          arXiv, Jun. 06, 2023.
          doi: <a href="https://doi.org/10.48550/arXiv.2303.03592">10.48550/arXiv.2303.03592</a>
        </li>
        <li id="cite-gao2020">
          Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, Connor Leahy (2020).
          <i>The {P}ile: An 800GB Dataset of Diverse Text for Language Modeling.</i>
          arXiv preprint, arXiv:2101.00027, 2020.
          [Online]. Available: <a href="https://arxiv.org/abs/2101.00027">https://arxiv.org/abs/2101.00027</a>
        </li>

      </ul>
    </section>
  </div>
</body>

</html>