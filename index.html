<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Witches Brew</title>
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Merriweather:wght@400;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <div class="blog-container">
        <h1 class="blog-title">An Analysis of: Witches’ Brew: Industrial Scale Data Poisoning Via Gradient Matching
        </h1>

        <h4> by Adrian Bisberg and Evan Rose </h4>

        <section class="blog-section magic-box" id="introduction">
            <h2>Introduction</h2>
            <p>
                Witches’ Brew introduces a novel type of integrity attack where a model is trained from scratch using tampered
                data in order to target a specific image for misclassification. The authors are motivated by creating an integrity attack
                that improves upon previous work to target larger scale models and work with minimal amounts of poisoned training
                data. <br><br>
                
                The technique used in their attack is called gradient matching. Here, the attacker
                mimics the gradient of the target by creating poisoned data whose training gradient aligns with the gradient of 
                an adversarial target. <br><br>
                
                The authors define a threat model of an attacker who can control a limited amount of the training
                data, cannot modify labels of the training data, and has knowledge of model parameters but not entire details of the 
                model architecture.

                <figure class="fig1">
                    <img src="media/poisoning_overview.png" alt="An overview of the poisoning procedure" class="centered-image">
                    <figcaption class="figcaption1">The attack perturbs a small set of data (around 0.1% in authors experiments)
                        in an imperceptable manner using parameter information from a pre-trained model. The poisoned data is then fed back for 
                        from-scratch training, and when the target image is checked for classification, the adversarial label is given.</figcaption>
                </figure>

                The data poisoning attack can be viewed as a bilevel optimization problem, where the poison perturbation &Theta; is optimized
                to enforce the malicious label onto target data, while the solution to the inner problem still minimizes the original 
                training objective of the model. Given P poisoned images by adding a perturbation &elta; to each image, the modification is
                constrained by the l<sub>&infin;</sub>-norm to be no less than hyperparameter &epsilon;. Within this constraint the goal 
                is to enforce similarity between the adversarial loss on targeted data x<sub>t</sub> (left side of equation) and the 
                training loss on poisoned data (right side of equation): 

                <figure class="fig1">
                    <img src="media/matching_gradient_eq.png" class="centered-image">
                    <figcaption class="figcaption1"> On the left side: the loss between adversarial label to predicted label of the targeted image. The goal is to minimize This
                        loss in order to misclassify the target image as the adversarial label. On the right side: the loss described between 
                        predicted label of poisoned data and its original label. Minimizing this loss maintains the validation accuracy of the model to 
                        keep the attack imperceptible.
                    </figcaption>
                </figure>

                Note: Here, authors decide to use a single target image x<sub>t</sub> for their experiments but mentioned multiple could be included 
                at once. However evidence of the effectiveness of using the method for multiple targets is not strong.<br><br>

                In order to align the target and the poison gradients in the same direction, the attack minimizes the cosine similarity between them using 
                signed Adam updates with a decaying step size. In this formulation, the model parameters &Theta; are not changed, instead &Delta;<sub>i</sub>
                is calculated in order to find the optimal poison to force misclassification. This method is particularly novel as it only requires a time
                budget of a single epoch, and the differentiation of the loss only requires a smaller subset of poisoned images P, rather than the entire 
                data set. Authors additionally give theoretical grounds for their method by invoking a proof using "Zoutendij" (as defined in Nocedal &
                Wright, 2006) to show that their bilevel gradient descent will converge locally. <br> <br>

                <figure class="fig1">
                    <img src="media/cosine_similarity.png" class="centered-image">
                    <figcaption class="figcaption1"> Plotting the cosine similarity across training epochs, comparing the poisoned image set versus the unmodified 
                        training to illustrate the gradient alignment achieved by the optimization process. The red line which represents that poisoned data maintains
                        a positive similarity throughout training. 
                    </figcaption>
                </figure>

                Because the threat model of the attack does not give full knowledge of the model architecture, such as dropout layers, hyperparameters, data 
                augmentation, etc, the authors propose several techniques to make to attack more robust to varying architecture: <br>
                1. Data augmentation - at each minimization step, randomly draw a translation crop, and possibly flip for each poisoned image. Creates 
                increased transferability and robustness to possible data augmentations within the model.<br>
                2. Restarts - minimize the cosine similarity several times using different random starting perturbations, then select the lowest captured
                alignment loss. <br>
                3. Model Ensembles - optimize the attack over an ensemble of different models, increasing generalized performance at a high computational cost.
                <br><br>

                <figure class="fig1">
                    <img src="media/graph_results.png" class="centered-image">
                    <figcaption class="figcaption1"> Results on CIFAR-10 for a ResNet-18 to visualize varied values for R (restarts) and K (number of ensembles). 
                    </figcaption>
                </figure>

                Authors display results for the accuracy of the poisoned data set in comparison to similar methods, as well as transfer results 
                that test on models that were not used for training the poisoned data set. 

                <figure class="fig1">
                    <img src="media/results_compared.png" class="centered-image">
                    <figcaption class="figcaption1"> Benchmark results using benchmark proposed in Schwarzschild et al. (2020). The category
                        "Training From Scratch" evaluates poisoned CIFAR-10 datasets with a poised training data budget of 1% 
                        and &epsilon; = 8 averaged over 100 fixed scenarios. Compares the results of previously proposed methods for data
                        poisoning from the same category. Models trained using ResNet-18 are evaluated next to other model architectures to 
                        demonstrate the transferability of the attack. 
                    </figcaption>
                </figure>

            </p>
        </section>

        <section class="blog-section magic-box" id="history">
            <h2>History and Related Works</h2>
            <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit...</p>
        </section>

        <section class="blog-section magic-box" id="author-biography">
            <h2>Author Biography</h2>
            <p>
                <div class="row">
                    <img src="media/jonas_geiping.png" class="photo">
                    <div class="description">
                        <h3>Jonas Geiping</h3>
                        <p> Education: MS in Mathematics from University of Münster; PhD in Computer Science from University of Siegen <br>
                            Jonas was a Postdoctoral Associate at University of Maryland during the publishing of this paper. <br>
                            Research goals: Understanding data privacy and adversarial attacks in large scale models. How to define and develop
                            "safer" models. 
                        </p>
                    </div>
                </div>
            
                <div class="row">
                    <img src="media/liam_fowl.jpg" class="photo">
                    <div class="description">
                        <h3>Liam Fowl</h3>
                        <p>Education: PhD in Mathematics from University of Maryland <br>
                           Research Interests: Clean-label data poisoning, metalearning, and federated learning</p>
                    </div>
                </div>

                <div class="row">
                    <img src="media/ronny_huang.jpg" class="photo">
                    <div class="description">
                        <h3>W Ronny Huang</h3>
                        <p>Education: PhD in Electrical Engineering and Computer Science at MIT, Machine Learning Researcher at University of Maryland <br>
                           Professional: Technical Staff at NASA, Research Scientist at Google </p>
                    </div>
                </div>

                <div class="row">
                    <img src="media/empty.jpg" class="photo">
                    <div class="description">
                        <h3>Wojciech Czaja</h3>
                        <p> Professor in the Mathematics department at University of Maryland <br> 
                            Author on over 100 papers at UMD, several collaborations with Geiping and Fowl</p>
                    </div>
                </div>

                <div class="row">
                    <img src="media/gavin_taylor.jpg" class="photo">
                    <div class="description">
                        <h3>Gavin Taylor</h3>
                        <p>Education: PhD from Duke University <br>
                           Professional: Professor of Computer Science at US Naval Academy, researcher in collaboration with 
                           UMD on the behaviors and vulnerabilities of neural networks
                        </p>
                    </div>
                </div>

                <div class="row">
                    <img src="media/michael_moller.jpg" class="photo">
                    <div class="description">
                        <h3>Michael Moeller</h3>
                        <p>Education: PhD from University of Münster <br>
                           Professional: Professor for Computer Vision at the University of Siegen, Germany <br>
                           Research Interests: Model and learning based techniques in imaging and computer vision
                        </p>
                    </div>
                </div>

                <div class="row">
                    <img src="media/tom_goldstein.png" class="photo">
                    <div class="description">
                        <h3>Tom Goldstein</h3>
                        <p>Education: PhD in Mathematics from UCLA, research scientist at Rice University and Stanford University<br>
                           Professional: Professor of Computer Science at University of Maryland<br>
                           Research Interests: Computer vision, signal processing, platform optimization
                        </p>
                    </div>
                </div>
            </p>
        </section>

        <section class="blog-section magic-box" id="societal-impact">
            <h2>Societal Impact</h2>
            <p> 
            Data poisoning attacks pose a serious security threat to the integrity of learning systems. The authors of Witches' Brew point out 
            the ideal type of poising attack includes imperceptible changes to images, minimal amounts of modified training data, and 
            no significant effect to the validation accuracy of the system. In machine learning systems that rely on large amounts of web scraped data, 
            attacks like Witches' Brew can easily be injected into several training cycles over time, making the attack both hard to detect
            and difficult to fix. There are always qualifications to the feasibility of an adversarial attack; for Witches' Brew the attack 
            does depend on availability of the trained model and access to contributing training data. This means the attack would be more effective 
            against a continual learning model or a reinforcement based model. <br><br>

            Model integrity attacks have larger social implications by providing a means to bypass important safety measures that the model is 
            trying to ensure. For example, an important measure taken in a large model could be to identify explicit or violent content, and a 
            data poisoning attack could force malicious classification of certain outputs as safe.  <br><br>

            Other types of adversarial attacks pose additional threats to model security. In a model extraction attack, the attacker uses queries of the 
            model and observed outputs in order to assemble a duplicate of a previously private model. Model inversion attacks can reveal information from the 
            data set that was used to train the model, and potentially compromise private information. <br><br>

            Research into adversarial attacks is useful for identifying model vulnerabilities to different types of attacks and evaluating the 
            effectiveness of various defensive strategies. When models are built with increased knowledge of different attacks, security
            measures can be incorporated into training and deployment of the model. In recent years, the field of adversarial research in ML 
            has grown significantly, providing for new standards to evaluate a model's robustness against adversarial attacks. Finding a completely 
            generalized defensive solution is difficult, however creating more fully documented and reproducible attacks doubtlessly will help
            unify and progress defensive strategies. 

            </p>
        </section>

        <section class="blog-section magic-box" id="industry-applications">
            <h2>Industry Applications</h2>
            <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit...</p>
        </section>

        <section class="blog-section magic-box" id="academic-research">
            <h2>Academic Research Follow-Ups</h2>
            <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit...</p>
        </section>

        <section class="blog-section magic-box" id="academic-research">
            <h2>Peer Review Ratings</h2>
            <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit...</p>
        </section>

        <section class="blog-section magic-box" id="references">
            <h2>References</h2>
            <p>

            Geiping, J., Fowl, L., Huang, W., Czaja, W., Taylor, G., Moeller, M., & Goldstein, T. (2021a). Witches’ Brew: Industrial Scale Data Poisoning via Gradient Matching. International Conference on Learning Representations. https://arxiv.org/pdf/2009.02276<br><br>
                
            Churilla, M., Vanhoudnos, N., & Beveridge, R. (2023, May 15). The Challenge of Adversarial Machine Learning. Carnagie Mellon Software Engineering. https://insights.sei.cmu.edu/blog/the-challenge-of-adversarial-machine-learning/<br><br>
            
            </p>
        </section>
    </div>
</body>
</html>
